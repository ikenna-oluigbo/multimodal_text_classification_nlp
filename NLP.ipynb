{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b8a88d-3f0c-4e61-a67d-6148f002c02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" \n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype='string')\n",
    "x = tf.keras.preprocessing.text.Tokenizer(inputs)\n",
    "x = tf.keras.layers.Embedding(x, 32)\n",
    "x = tf.keras.layers.LSTM(64, activation='tanh')(x)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "mod = tf.keras.Model(inputs, output, name='LSTM_TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6433566b-3e60-469e-be14-b6a61a110d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" \n",
    "\n",
    "train_df = pd.read_csv('./nlp/train.csv')\n",
    "test_df = pd.read_csv('./nlp/test.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5983a9a-cd61-4f8a-9c1a-508e6904d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset. Ony training dataset is shuffled\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42)\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da620d9-1580-4658-b77a-898778b166bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f37c8-cee2-471e-9999-903cdc4aaa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''My Version'''\n",
    "\n",
    "import random \n",
    "ind = random.sample(range(len(train_df_shuffled)), 5)\n",
    "X = train_df_shuffled[['text', 'target']]\n",
    "for i in ind:\n",
    "    print(X.iloc[i])\n",
    "    print('Disaster' if X.iloc[i].target == 1 else 'Not Disaster')\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ff0fa1-4fb3-49f8-92a1-eb59522560a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize random samples\n",
    "import random \n",
    "random_indexes = [random.randint(0, len(train_df_shuffled)) for i in range(5)]\n",
    "print(random_indexes)\n",
    "for i in random_indexes:\n",
    "    x = train_df_shuffled[['text', 'target']].iloc[i]\n",
    "    print('Target {}:'.format(x[1]), \"Disaster\" if x[1]==1 else \"Not Disaster\")\n",
    "    print(x, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01a3fce-b2d7-4814-825a-5acd79ede695",
   "metadata": {},
   "source": [
    "# Split data into training and validation sets from train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7745c6-b3f1-434e-8a46-b20c9ebb33b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "train_sentences, val_sentences, train_labels, val_labels = tts(train_df_shuffled['text'].to_numpy(), \n",
    "                                                               train_df_shuffled['target'].to_numpy(),\n",
    "                                                               test_size=0.1, random_state=42)\n",
    "print(train_sentences[:5], train_labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d0893-7def-4956-a466-d2b7424b6f87",
   "metadata": {},
   "source": [
    "## Next step will be to tokenize the text before embedding. To do this, we use Textvectorizer. \n",
    "## Note: You can also use the Text tokenizer from preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f4d5a8-a689-40f6-8447-5248f6c50647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we know the total words in the training vocabulary\n",
    "words_size = sum([len(i.split()) for i in train_sentences]) #Total words\n",
    "avg_words = round(words_size / len(train_sentences)) #Bcos of unequal lenght of each tweet, we get the avg per tweet to help build vectorizer parameters\n",
    "print(words_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce35c7-51c5-4ac5-9cc6-17fa544d12e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=int(words_size/2), #This is the max words to consider in the vocab. Set to None if pad_to_max_tokens=False\n",
    "                                                    standardize='lower_and_strip_punctuation',\n",
    "                                                    ngrams=None, \n",
    "                                                    split='whitespace', \n",
    "                                                    output_mode='int',\n",
    "                                                    output_sequence_length=avg_words,\n",
    "                                                    pad_to_max_tokens=True) #Adds 0 to sentences not up to output sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e767a1c0-a6a3-41d9-aa0e-629d8d84540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit text vectorizer to training data\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2a15fa-76b7-48d8-b69f-99346f8db6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the text vectorizer\n",
    "samp = \"The best coin is FLUX and STRATOS token is the worst token\"\n",
    "random_sentence = random.choice(train_sentences)\n",
    "token_random_sentence = text_vectorizer(random_sentence)\n",
    "print(random_sentence, '\\n')\n",
    "print(\"Length Random Sentence:\", len(random_sentence.split()))\n",
    "print(token_random_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eb3359-3eb6-4702-adce-4ac6edfad69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Unique words in the vocabulary\n",
    "unique_words = text_vectorizer.get_vocabulary()\n",
    "Ten_most_common_words = [str(u) for u in unique_words[:30]]\n",
    "Ten_least_common_words = unique_words[-10:]\n",
    "print(len(unique_words))\n",
    "print('10 most common words: {}'.format(Ten_most_common_words))\n",
    "print('10 least common words: {}'.format(Ten_least_common_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec3d76f-a315-4bb0-a937-54972042b20c",
   "metadata": {},
   "source": [
    "# Create Embedding using the Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db742833-7258-4e45-85ec-ee38391a6c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = int(words_size/2) \n",
    "out_dim = 128 \n",
    "in_len = avg_words\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(input_dim=in_dim,\n",
    "                                      output_dim=out_dim,\n",
    "                                      input_length=in_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d7ea62-9dca-406a-b5b1-b90dee515c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sentence = random.choice(train_sentences)\n",
    "print('Original sentence: {}'.format(random_sentence))\n",
    "random_vec = text_vectorizer(random_sentence)\n",
    "print('Numeric Vectors: {}'.format(random_vec))\n",
    "sentence_embeddings = embedding(random_vec)\n",
    "print(sentence_embeddings, len(sentence_embeddings))\n",
    "print(sentence_embeddings[0], sentence_embeddings[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65701bec-0445-4aac-9821-2f0b6a19701b",
   "metadata": {},
   "source": [
    "# Tensorboard Callback Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95eaac6-28c3-433e-877d-9bc2929dace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "SAVE_DIR = \"model_logs\"       #Directory to save all model logs \n",
    "\n",
    "def create_tensorboard_callback(dir_name, experiment_name):\n",
    "  \"\"\"\n",
    "  Creates a TensorBoard callback instand to store log files.\n",
    "\n",
    "  Stores log files with the filepath:\n",
    "    \"dir_name/experiment_name/current_datetime/\"\n",
    "\n",
    "  Args:\n",
    "    dir_name: target directory to store TensorBoard log files\n",
    "    experiment_name: name of experiment directory (e.g. efficientnet_model_1)\n",
    "  \"\"\"\n",
    "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "      log_dir=log_dir\n",
    "  )\n",
    "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
    "  return tensorboard_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375c949-59e3-4267-b496-7d4f47703d49",
   "metadata": {},
   "source": [
    "# Modelling a series of experiments \n",
    "\n",
    ". Create a model\n",
    ". Build a model \n",
    ". Fit a model \n",
    ". Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee3ff37-dd27-4f8f-872e-b222a847ff1b",
   "metadata": {},
   "source": [
    "### First Model = We start with a baseline : Naives Bayes with TF-IDF - Model 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da21673-7d32-453d-b957-52f18c683652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Model = We start with a baseline : Naives Bayes with TF-IDF - Model 0\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "model_0 = Pipeline([\n",
    "                    (\"tfidf\", TfidfVectorizer()),\n",
    "                    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "model_0.fit(train_sentences, train_labels)\n",
    "\n",
    "baseline_score = model_0.score(val_sentences, val_labels)\n",
    "print(\"Our baseline model achieves accuracy of {:.2f}%\".format(baseline_score*100))\n",
    "\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "print(baseline_preds[:10])\n",
    "print(val_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a467d0-a222-4c4b-be9a-cd6b77f883b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate: accuracy, precision, recall, and F1 score \n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support \n",
    "\n",
    "def calculate_results(y_true, y_pred):\n",
    "    \"\"\"\n",
    "        Calculates the accuracy, precision, recall, and F1 score for the classification model. \n",
    "    \"\"\"\n",
    "    model_accuracy = accuracy_score(y_true, y_pred) * 100 \n",
    "    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    model_results = {\"Accuracy\": model_accuracy, \n",
    "                     \"Precision\": model_precision, \n",
    "                     \"Recall\": model_recall,\n",
    "                     \"F1\": model_f1}\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c43f1e-cd3e-4d85-8394-38ee827aea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Baseline results \n",
    "baseline_results = calculate_results(y_true=val_labels, y_pred=baseline_preds)\n",
    "print(baseline_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04004171-ad52-401d-be1b-92d2146596b6",
   "metadata": {},
   "source": [
    "# Second Model = A Simple Dense Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7bf72e-2ed0-41ad-b50d-01d0b18a19e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second Model = Feed forward neural network (dense model)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.Dense(10, activation='relu')(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x) #condense the feature vector for each token to one vector\n",
    "#x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"Model_1_FFNN\")\n",
    "\n",
    "model_1.summary()\n",
    "\n",
    "model_1.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model_1_history = model_1.fit(x=train_sentences, \n",
    "            y=train_labels,\n",
    "            epochs=10,\n",
    "            validation_data=(val_sentences, val_labels), \n",
    "            callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
    "                                                 experiment_name='model_1_simple_dense')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2336fa-b80f-47e8-ad61-e047929b8e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_1.evaluate(val_sentences, val_labels)\n",
    "print(\"Model_1 Loss\", loss, \" |||| \", \"Model_1_Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e29946-f5e0-418e-b372-6e711629e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_pred = model_1.predict(val_sentences) #This give probabilities, we need to convert to numeric values\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred))\n",
    "\n",
    "model_1_results = calculate_results(y_true=val_labels, y_pred=model_1_preds)\n",
    "print(model_1_results)\n",
    "print(baseline_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5241c88d-e97a-4b89-aebe-45a634781935",
   "metadata": {},
   "source": [
    "## Visualizing Learned Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b40a1e-328c-4e22-9610-49464b4e640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "print(len(words_in_vocab), words_in_vocab[:10])\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e90b2c0-da41-4444-9c85-d86525d04048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the weight matrix of the embedding layer for model 1\n",
    "embed_weights = model_1.get_layer('embedding_3').get_weights()[0]\n",
    "print(embed_weights, embed_weights.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2d2cd2-5b2e-4907-8c40-edc7b01f0204",
   "metadata": {},
   "source": [
    "## This code downloads the vectors and metadata tsv file. Visualize on the tensorflow projector\n",
    "### https://projector.tensorflow.org/   (Load the vectors and metasata tsv in Tensorflow projector)\n",
    "### https://www.tensorflow.org/text/guide/word_embeddings   (Read about word embedding here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5affcc-aa1e-4cfa-aede-16ae1e5d9acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code downloads the vectors and metadata tsv file. Visualize on the tensorflow projector\n",
    "#https://projector.tensorflow.org/   (Load the vectors and metasata tsv in Tensorflow projector)\n",
    "#https://www.tensorflow.org/text/guide/word_embeddings   (Read about word embedding here)\n",
    "import io\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(words_in_vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = embed_weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48e87f-ef26-4ccd-8978-10bf72b14f0b",
   "metadata": {},
   "source": [
    "# Third Model - Recurrent Neural Network\n",
    "#### RNN's are useful for sequence data. It uses the representation of a previous input to aid the representation of a later input \n",
    "#### Resources Includes: \n",
    "#### MIT's sequence modelling lecture https://authurwhywait.github.io/blog/2021/12/02/introduction_to_dl02/ \n",
    "#### Chrish Olah's intro to LSTMs : https://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n",
    "#### Andrej karparthy's The Unreasonable effectiveness of recurrent neural network : https://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f3b4c-b7d3-4fda-a19e-b9bfe9254689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LSTM model \n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype='string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.LSTM(64, return_sequences=True)(x)  #When stacking LSTM layers, the first LSTM layer has to have return_sequences=True\n",
    "x = tf.keras.layers.LSTM(64, activation='tanh')(x)\n",
    "#x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs, name='LSTM_model')\n",
    "\n",
    "model_2.summary()\n",
    "\n",
    "model_2.compile(\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "            optimizer=tf.keras.optimizers.Adam(),\n",
    "            metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_2_history = model_2.fit(\n",
    "                        train_sentences,\n",
    "                        train_labels,\n",
    "                        epochs=10,\n",
    "                        validation_data=(val_sentences, val_labels), \n",
    "                        callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
    "                                                 experiment_name='model_2_lstm')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef28be3-a8bf-411a-b842-2c4052a22d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_preds = model_2.predict(val_sentences)\n",
    "model_2_preds = tf.squeeze(tf.round(model_2_preds))\n",
    "print(np.array(model_2_preds[:10]))\n",
    "print(np.array(model_1_preds[:10]))\n",
    "print(baseline_preds[:10])\n",
    "print(val_labels[:10])\n",
    "\n",
    "model_2_results = calculate_results(val_labels, model_2_preds)\n",
    "print(model_2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd60838d-caf3-42ea-a9ef-e4c538a44eaa",
   "metadata": {},
   "source": [
    "# Fourth Model - Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011c02bc-d894-4b44-ade2-b5335101e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session \n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype='string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.GRU(64, return_sequences=True)(x)  #When stacking GRU layers, the first LSTM layer has to have return_sequences=True\n",
    "#x = tf.keras.layers.LSTM(64, return_sequences=True)(x)  #You can stack GRU cell ontop of LSTM and vice versa\n",
    "x = tf.keras.layers.GRU(64, activation='tanh')(x) \n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model_3 = tf.keras.Model(inputs, outputs, name='GRU_model')\n",
    "\n",
    "model_3.summary()\n",
    "\n",
    "model_3.compile(\n",
    "                loss = tf.keras.losses.BinaryCrossentropy(), \n",
    "                optimizer = tf.keras.optimizers.Adam(), \n",
    "                metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model_3_history = model_3.fit(train_sentences, train_labels,\n",
    "                              epochs = 10,\n",
    "                              validation_data=(val_sentences, val_labels), \n",
    "                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
    "                                                 experiment_name='model_3_gru')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45daa0a6-7330-47b2-8035-88bda7f18cad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_3_preds = model_3.predict(val_sentences)\n",
    "model_3_preds = tf.squeeze(tf.round(model_3_preds))\n",
    "print(model_3_preds[:10])\n",
    "model_3_results = calculate_results(val_labels, model_3_preds)\n",
    "print(model_3_results)\n",
    "print(baseline_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ce2a9c-90b0-4717-9e8d-265facbd29f9",
   "metadata": {},
   "source": [
    "# Fifth Model - Bidirectional RNN \n",
    "\n",
    "#### Normal RNN goes from left to right. Bidirectional combines representation from right to left as well as from left to right \n",
    "#### So basically, it encodes text from one direction to the next, and from the other direction to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7848ee-7646-4bba-8e0a-b22b5c1551fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype='string') \n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x)  #Bidirectional doubles the shape sequence bcos it goes both ways (64*2)\n",
    "#x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True))(x)\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, activation='tanh'))(x)  #GRU and LSTM can be interchanged or even used together\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model_4 = tf.keras.Model(inputs, outputs, name='Bidirectional_Model')\n",
    "\n",
    "model_4.summary()\n",
    "\n",
    "model_4.compile(\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_4_history = model_4.fit(train_sentences, train_labels,\n",
    "                              epochs=10,\n",
    "                              validation_data=(val_sentences, val_labels), \n",
    "                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
    "                                                 experiment_name='model_4_bidirectional_lstm')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42247e6c-7c74-42e3-83d1-4f0ff1774f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4_preds = model_4.predict(val_sentences)\n",
    "model_4_preds = tf.squeeze(tf.round(model_4_preds))\n",
    "print(model_4_preds[:10])\n",
    "model_4_results = calculate_results(val_labels, model_4_preds)\n",
    "print(model_4_results)\n",
    "print(baseline_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35b00de-f19a-454f-935b-37bc326dd0b4",
   "metadata": {},
   "source": [
    "# Bidirectional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d61bbc6-dda7-40cb-98c6-f5c0d98c2e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True))(x)\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, activation='tanh'))(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model_4_5 = tf.keras.Model(inputs, outputs, name='Bidirectional_Model_GRU')\n",
    "\n",
    "model_4_5.compile(\n",
    "            loss = tf.keras.losses.BinaryCrossentropy(), \n",
    "            optimizer = tf.keras.optimizers.Adam(), \n",
    "            metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model_4_5_history = model_4_5.fit(train_sentences, train_labels, \n",
    "                                  epochs=10, \n",
    "                                  validation_data = (val_sentences, val_labels), \n",
    "                                  callbacks = [create_tensorboard_callback(dir_name=SAVE_DIR,\n",
    "                                                 experiment_name='model_4_bidirectional_gru')]\n",
    ")\n",
    "\n",
    "model_4_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f783f-5871-41ff-a53f-77036bad46e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4_5_preds = model_4_5.predict(val_sentences)\n",
    "model_4_5_preds = tf.squeeze(tf.round(model_4_5_preds))\n",
    "\n",
    "model_4_5_results = calculate_results(val_labels, model_4_5_preds)\n",
    "model_4_5_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da0ad07-6ad8-401a-b69a-f5786ddd5fe5",
   "metadata": {},
   "source": [
    "# Sixth Model - Convolutional Neural Network for Text (and other types of sequences) \n",
    "#### Conv2D is used for image data, while Conv1D is applicable to texts with one dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e6bcbc-3270-4afd-b616-0b4e4222abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us first do some testing\n",
    "\n",
    "embedding_test = embedding(text_vectorizer(['My name is Ikenna Oluigbo and i have two sons']))\n",
    "conv_1d = tf.keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, activation='relu', padding='same')\n",
    "conv_1d_output = conv_1d(embedding_test)\n",
    "max_pool = tf.keras.layers.GlobalMaxPool1D()\n",
    "max_pool_output = max_pool(conv_1d_output)\n",
    "\n",
    "#embedding_test.shape, conv_1d_output.shape, max_pool_output.shape \n",
    "print(max_pool_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b8c63-1ab9-4581-9b00-39caae35cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session \n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype='string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.Conv1D(filters=64, kernel_size=5, padding='valid', activation='relu')(x)\n",
    "x = tf.keras.layers.GlobalMaxPool1D()(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model_5 = tf.keras.Model(inputs, outputs, name='Conv1D_Model')\n",
    "\n",
    "model_5.summary()\n",
    "\n",
    "model_5.compile(\n",
    "                loss = tf.keras.losses.BinaryCrossentropy(), \n",
    "                optimizer = tf.keras.optimizers.Adam(), \n",
    "                metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model_5_history = model_5.fit(train_sentences, train_labels,\n",
    "                              epochs=10, \n",
    "                              validation_data=(val_sentences, val_labels), \n",
    "                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
    "                                                 experiment_name='model_5_conv1d')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5c5561-fb7b-4993-93a7-d135b438ba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5_preds = model_5.predict(val_sentences)\n",
    "model_5_preds = tf.squeeze(tf.round(model_5_preds))\n",
    "print(model_5_preds[:10])\n",
    "model_5_results = calculate_results(val_labels, model_5_preds)\n",
    "print(model_5_results)\n",
    "print(baseline_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d6dc75-e4e7-4f16-bead-39c44b4fc1a3",
   "metadata": {},
   "source": [
    "# Pretrained Model using Universal Sentence Encoder in Tensorflow Hub \n",
    "#### We have built our own models, let us try transfer learning for NLP \n",
    "#### https://www.kaggle.com/models/google/universal-sentence-encoder/tensorFlow2/universal-sentence-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bbb5ac-6fe8-4816-bf51-2d6c82502738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a sample to demonstrate how the pretrained model work \n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "embed = hub.load(\"https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2\") \n",
    "embed_samples = embed([samp, \n",
    "                 random_sentence])\n",
    "\n",
    "print(embed_samples[0][:50])\n",
    "\n",
    "tt = 'Will Isak sign for Liverpool or remain at Newcastle'\n",
    "ttt = embed([tt])\n",
    "print('\\n')\n",
    "print(ttt[0][:50])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dc3c20-eee9-4243-823e-7af0bac9c3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf \n",
    "\n",
    "#Create a Keras Layer using the USE pretrained layer from tensorflow hub\n",
    "sentence_encoder_layer = hub.KerasLayer(\"https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2\",\n",
    "                                       input_shape = [],  #empty because the pretrained model can take variable lenght inputs\n",
    "                                       trainable = False,  #false so as not to train the embeddings\n",
    "                                       dtype = tf.string, \n",
    "                                       name = 'USE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d053dd01-d3ab-4113-a6a1-810a6797963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model using the Sequential API \n",
    "## For using a pretrained model, use tf_keras instead of tf.keras\n",
    "\n",
    "import tf_keras\n",
    "\n",
    "'''Repeated this here because pretrained models need tf_keras'''\n",
    "\n",
    "import datetime\n",
    "\n",
    "#SAVE_DIR = \"model_logs\"       #Directory to save all model logs \n",
    "\n",
    "def create_tensorboard_callback_pretrained(dir_name, experiment_name):\n",
    "  \"\"\"\n",
    "  Creates a TensorBoard callback instand to store log files.\n",
    "\n",
    "  Stores log files with the filepath:\n",
    "    \"dir_name/experiment_name/current_datetime/\"\n",
    "\n",
    "  Args:\n",
    "    dir_name: target directory to store TensorBoard log files\n",
    "    experiment_name: name of experiment directory (e.g. efficientnet_model_1)\n",
    "  \"\"\"\n",
    "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "  tensorboard_callback = tf_keras.callbacks.TensorBoard(\n",
    "      log_dir=log_dir\n",
    "  )\n",
    "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
    "  return tensorboard_callback\n",
    "\n",
    "\n",
    "\n",
    "model_6 = tf_keras.Sequential(name='model_6_USE')\n",
    "model_6.add(sentence_encoder_layer) \n",
    "model_6.add(tf_keras.layers.Dense(64, activation='relu'))  #Help upgrade  the result\n",
    "model_6.add(tf_keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_6.summary()\n",
    "\n",
    "model_6.compile(\n",
    "            loss = tf_keras.losses.BinaryCrossentropy(),\n",
    "            optimizer = tf_keras.optimizers.Adam(),\n",
    "            metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model_6_history = model_6.fit(train_sentences, train_labels, \n",
    "                              epochs = 10,\n",
    "                              validation_data = (val_sentences, val_labels), \n",
    "                              callbacks=[create_tensorboard_callback_pretrained(dir_name=SAVE_DIR,\n",
    "                                                 experiment_name='model_6_tfhub_USE')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f0241d-d19e-477d-bcec-8a8e3e5650eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6_preds = model_6.predict(val_sentences)\n",
    "model_6_preds = tf.squeeze(tf.round(model_6_preds))\n",
    "\n",
    "model_6_results = calculate_results(val_labels, model_6_preds)\n",
    "print(model_6_results)\n",
    "print(baseline_results)\n",
    "print('\\n')\n",
    "print(model_6_preds[:20])\n",
    "print(val_labels[:20])\n",
    "print(model_6_preds[:20] == val_labels[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e881976-260e-48f1-93c4-808e68d12eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Experimenting with BERT'''\n",
    "\n",
    "import tensorflow_hub as hub \n",
    "import tf_keras\n",
    "import tensorflow_text\n",
    "\n",
    "text_input = tf_keras.layers.Input(shape=(), dtype=tf.string)\n",
    "preprocessor = hub.KerasLayer(\n",
    "    \"https://kaggle.com/models/tensorflow/bert/TensorFlow2/multi-cased-preprocess/3\")\n",
    "encoder_inputs = preprocessor(text_input)\n",
    "\n",
    "encoder = hub.KerasLayer(\n",
    "    \"https://www.kaggle.com/models/tensorflow/bert/TensorFlow2/multi-cased-l-12-h-768-a-12/4\",\n",
    "    trainable=True)\n",
    "outputs = encoder(encoder_inputs)\n",
    "pooled_output = outputs[\"pooled_output\"]      # [batch_size, 768].\n",
    "sequence_output = outputs[\"sequence_output\"]  # [batch_size, seq_length, 768].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a018fa9-e88e-41fb-a27b-5d31ff4d7264",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = tf_keras.Model(text_input, pooled_output)\n",
    "sentences = tf.constant([\"Will Liverpool sign Isak or will he remain at Newcastle?\"])\n",
    "#print(embedding_model(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23a2485-acd5-465f-ba79-228e3fa04a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bert = tf_keras.Sequential()\n",
    "x = tf_keras.layers.Dense(64, activation='relu')(pooled_output)\n",
    "x = tf_keras.layers.Dense(32, activation='relu')(x)\n",
    "x = tf_keras.layers.Dense(16, activation='relu')(x)\n",
    "output = tf_keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model_bert = tf_keras.Model(text_input, output, name='test_BERT')\n",
    "\n",
    "model_bert.compile(loss=tf_keras.losses.BinaryCrossentropy(),\n",
    "                   optimizer=tf_keras.optimizers.Adam(), \n",
    "                   metrics=['acc'])\n",
    "\n",
    "model_bert.fit(train_sentences, train_labels, epochs=2,\n",
    "               validation_data=(val_sentences, val_labels))\n",
    "\n",
    "model_bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d5fc0-584d-47f7-af30-df4b7a0f268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pred = model_bert.predict(val_sentences)\n",
    "bert_pred = tf.squeeze(tf.round(bert_pred))\n",
    "print(calculate_results(val_labels, bert_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecd8ef4-8a30-4d84-99dc-89c8b707d81b",
   "metadata": {},
   "source": [
    "# TF Hub Pretrained USE but with 10% of training data\n",
    "#### Transfer learning really helps when you dont have large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a796a2-d2cc-4124-9ba4-9dbc714bf69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 10% of the training data set \n",
    "# Can you quickly spot a problem here? There will be data leakage since some sentences in train_10_percent will be in val sentences\n",
    "\n",
    "train_10_percent = train_df_shuffled[['text', 'target']].sample(frac=0.1, random_state=42)\n",
    "train_sentences_10_percent = train_10_percent['text'].to_numpy()\n",
    "train_labels_10_percent = train_10_percent['target'].to_numpy()\n",
    "print(train_10_percent['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f6d04d-8957-45d4-9b12-4bfb0af47884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We try to fix data leakage by taking 10 percent from the original training sentences \n",
    "\n",
    "total_len = len(train_sentences)\n",
    "train_percent = int(total_len * 0.1)   #Ten percent\n",
    "train_sentences_10percent_shuffle = train_sentences[:train_percent]\n",
    "train_labels_10percent_shuffle = train_labels[:train_percent]\n",
    "\n",
    "dt = pd.Series(train_labels_10percent_shuffle)\n",
    "dt.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e12d93-8e0a-4b8c-af61-2ec9f1acea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a model the same to model 6\n",
    "#We can either retype the model same as in model 6 or we can just clone the model \n",
    "# Since we are still on the matter of using pretrained model, remember to use the tf_keras\n",
    "\n",
    "model_7 = tf_keras.models.clone_model(model_6)\n",
    "model_7._name = 'model_7_USE_10percent_data'   #Rename the model\n",
    "\n",
    "model_7.compile(\n",
    "            loss = tf_keras.losses.BinaryCrossentropy(),\n",
    "            optimizer = tf_keras.optimizers.Adam(), \n",
    "            metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model_7.fit(\n",
    "            train_sentences_10percent_shuffle, train_labels_10percent_shuffle, \n",
    "            #train_sentences_10_percent, train_labels_10_percent,\n",
    "            epochs = 10, \n",
    "            validation_data=(val_sentences, val_labels), \n",
    "            callbacks=[create_tensorboard_callback_pretrained(dir_name=SAVE_DIR,\n",
    "                                                 experiment_name='model_7_tfhub_USE_10percent')]\n",
    ")\n",
    "\n",
    "model_7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4af684-9e02-4fb6-834a-7b4e25b28a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_7_preds = model_7.predict(val_sentences)\n",
    "model_7_preds = tf.squeeze(tf.round(model_7_preds))\n",
    "print(model_7_preds[:10])\n",
    "\n",
    "model_7_results = calculate_results(val_labels, model_7_preds)\n",
    "print(model_7_results) \n",
    "print(model_6_results)\n",
    "#Interesting that model 7 outperforms model 6 even though it is trained on 10% less data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c41a5a-1500-4269-9880-66664ba3840b",
   "metadata": {},
   "source": [
    "# Comparing the Performance of each of out models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3850f84b-16f7-4d89-b4c4-be4f9ecccee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_dict = dict()\n",
    "all_models = [baseline_results, model_1_results, model_2_results, model_3_results, \n",
    "              model_4_results, model_4_5_results, model_5_results, model_6_results, model_7_results]\n",
    "all_index = [\"baseline_results\", \"model_1_results\", \"model_2_results\", \"model_3_results\", \n",
    "              \"model_4_results\", \"model_4_5_results\", \"model_5_results\", \"model_6_results\", \"model_7_results\"]\n",
    "\n",
    "acc = list() ; pre = list() ; rec = list() ; fscore = list() \n",
    "\n",
    "for i in all_models: \n",
    "    acc.append(round(i['Accuracy'], 3))\n",
    "    pre.append(round(i['Precision'], 3))\n",
    "    rec.append(round(i['Recall'], 3))\n",
    "    fscore.append(round(i['F1'], 3))\n",
    "\n",
    "all_model_dict['Accuracy_Score'] = acc\n",
    "all_model_dict['Precision_Score'] = pre\n",
    "all_model_dict['Recall_Score'] = rec\n",
    "all_model_dict['F1_Score'] = fscore\n",
    "\n",
    "models_performance = pd.DataFrame(all_model_dict, index=all_index)\n",
    "models_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1a2b4e-9c53-456a-9aee-5ee29ff87d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALternative way of putting all model result in table\n",
    "\n",
    "model_dict = pd.DataFrame({\"0_baseline\" : baseline_results, \n",
    "                           \"1_simple_dense\" : model_1_results,\n",
    "                           \"2_lstm\" : model_2_results,\n",
    "                           \"3_gru\" : model_3_results,\n",
    "                           \"4_bidirectional_lstm\" : model_4_results,\n",
    "                           \"4_5_bidirectional_gru\" : model_4_5_results,\n",
    "                           \"5_conv1d\" : model_5_results,\n",
    "                           \"6_tfhub_use_encoder\" : model_6_results,\n",
    "                           \"7_tfhub_use_encoder_10percent\" : model_7_results})\n",
    "\n",
    "# Put Accuracy on thesame scale as others\n",
    "model_dict.loc['Accuracy'] = model_dict.loc['Accuracy'] / 100\n",
    "model_dict = model_dict.transpose()\n",
    "model_dict              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1e351d-d8e3-483e-86e7-1dcf67ca4651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing results\n",
    "\n",
    "model_dict.plot(kind='bar', figsize=(10,7)).legend(bbox_to_anchor=(1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad92fe0-d4fe-4843-810a-f9c2834a3cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict['Accuracy'].sort_values(ascending=False).plot(kind='bar', figsize=(10,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5426f2af-9af0-4ded-a139-eac8ec65c404",
   "metadata": {},
   "source": [
    "# Uploading out model training logs to Tensorboard.dev \n",
    "#### Inspect it using tensorboard.dev: https://tensorboard.dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705592f9-468f-486d-aa16-1f5018da3dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" View Model Logs on Tensorboard \"\"\"\n",
    "\n",
    "!tensorboard --logdir model_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be31692-b36f-41fc-87a0-a0b24cd30c5a",
   "metadata": {},
   "source": [
    "# Saving Models \n",
    "### The two main formats to save tensorflow models: \n",
    "#### 1. The HDF5 format \n",
    "#### 2. The SavedModel format (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7a84d4-53dd-482c-96d9-18dc62ab0e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us save the best performing model, Model 6 (TF Hub Pretrained)\n",
    "\n",
    "model_6.save(\"model_6.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b58565-678e-4798-a7c8-436e07fa5b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model \n",
    "import tensorflow_hub as hub\n",
    "\n",
    "loaded_model6 = tf_keras.models.load_model(\"model_6.h5\", \n",
    "                                          custom_objects={\"KerasLayer\" : hub.KerasLayer})  #Custom Objects is needed for h5 format\n",
    "loaded_model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a7bf3a-b91b-4faa-b474-489824579ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying the second SavedModel format \n",
    "\n",
    "model_6.save(\"model_6_savedformat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2a22bc-267d-4c27-b3be-3670d8f77b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model6_savedformat = tf_keras.models.load_model(\"model_6_savedformat\")\n",
    "loaded_model6_savedformat.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dd58fe-3065-4da0-a1ba-2b69e7c84339",
   "metadata": {},
   "source": [
    "# Model driven data exploration also known as ACTIVE LEARNING \n",
    "#### Using a trained model to visualize the data \n",
    "#### 0 = Not Disaster |  1 = Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588270c-5d41-493c-828c-64c4d5a202c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6_preds_prob = tf.squeeze(model_6.predict(val_sentences))  #Just so we have probabilities\n",
    "\n",
    "val_df = pd.DataFrame({\"text\" : val_sentences, \n",
    "                      \"target\" : val_labels, \n",
    "                      \"pred\" : model_6_preds, \n",
    "                      \"pred_prob\" : model_6_preds_prob})\n",
    "\n",
    "#val_df.head()\n",
    "\n",
    "\"\"\" Let us find the wrong predictions and sort by predictions probabilities \"\"\" \n",
    "\n",
    "most_wrong = val_df[val_df['target'] != val_df['pred']].sort_values('pred_prob', ascending=False)\n",
    "\n",
    "num = 5\n",
    "l = len(most_wrong)\n",
    "for i in range(num):\n",
    "    curr = random.choice(range(l))\n",
    "    text, target, pred, pred_prob = most_wrong.iloc[curr].items()\n",
    "    print(f\"Target: {target[1]}, Pred: {pred[1]}, Prob: {pred_prob[1]}\")\n",
    "    print(\"Text: \", text[1])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af46dec8-648e-40aa-a2cd-50e9ad88720a",
   "metadata": {},
   "source": [
    "# Making Predictions on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155d6dfc-c7a2-455b-90dd-06c2cbc60394",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = test_df['text'].to_numpy()\n",
    "length_test = len(test_sentences)\n",
    "num_p = 5\n",
    "for i in range(num_p):\n",
    "    c = random.choice(range(length_test))\n",
    "    t = test_sentences[c]\n",
    "    test_pred_prob = model_6.predict([t])\n",
    "    test_pred = tf.squeeze(tf.round(test_pred_prob))\n",
    "    print(f\"Pred: {test_pred}, Prob: {test_pred_prob[0]}\")\n",
    "    print('Text: ', t)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777fb681-05da-415f-a28e-bfeb111f7acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Making predictions from live tweets\"\"\"\n",
    "\n",
    "tt = model_6.predict(['We escaped a bandit attack ystdy at around 2pm 20km from Talata-Mafara.'])\n",
    "print(tt)\n",
    "print(tf.squeeze(tf.round(tt)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e051208-7f76-467e-9c06-6e96efd14498",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = test_df['text'].to_numpy()\n",
    "test_id = test_df['id'].to_numpy()\n",
    "pred_res = [int(i) for i in tf.squeeze(tf.round(model_6.predict([test_sentences])))]\n",
    "res_df = pd.DataFrame({'id' : test_id, \n",
    "                       'target' : pred_res})\n",
    "res_df.to_csv('NLP with Disaster Tweet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
